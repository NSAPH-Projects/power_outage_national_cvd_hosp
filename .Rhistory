# want min number of chunks without exceeding the limit for max performance
# going with ~ 150 chunks
pous_list <- sort(unique(pous_data$five_digit_fips))
pous_l_split <- split(pous_list, ceiling(seq_along(pous_list) / 15))
i = 1
# separate chunk
pous_dat_chunk <- pous_data[five_digit_fips %in% pous_l_split[[i]]]
pous_dat_chunk <- pous_dat_chunk[, year := lubridate::year(recorded_date_time)]
pous_dat_chunk <- pous_dat_chunk[year == 2018]
# make id for city-utility-year
make_id <- unique(
pous_dat_chunk[, .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name)],
by = c(
"clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name"
)
)
# separate chunk
pous_dat_chunk <- pous_data[five_digit_fips %in% pous_l_split[[i]]]
pous_dat_chunk <- pous_dat_chunk[, year := lubridate::year(recorded_date_time)]
pous_dat_chunk <- pous_dat_chunk %>% rename(fips = five_digit_fips)
# make id for city-utility-year
make_id <- unique(
pous_dat_chunk[, .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name)],
by = c(
"clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name"
)
)
# make year data.table
years <- data.table(year = c(2017, 2018, 2019, 2020))
# expand this to a datatable containing one entry for every city-utility
# and year, so four entries per city-utility
make_id <-
make_id[, .(year = years$year), by = .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name)]
# add unique ID
make_id <- make_id[, city_utility_name_id := 1:.N]
# expand the datatable into a timeseries of intervals for every year - SLOW -----
city_utility_time_series <-
make_id[, .(date = intervals_dt$date), by = .(
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
city_utility_name_id
)]
glimpse(city_utility_time_series)
# order the pous data.table so we can correctly identify missing
# observation indicators
pous_dat_chunk <- pous_dat_chunk[order(
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
recorded_date_time,
-customers_out
)]
# index observations to identify matching time stamps which are missing ob ind
pous_dat_chunk[, index := 1:.N, by = c(
"clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name",
"recorded_date_time"
)]
# replace missing indicators with a more obvious missingness indicator, NA,
# and move them to the right spot in the time series
pous_dat_chunk[index == 2, customers_out := -99]
# remove duplicate rows (those are errors)
pous_dat_chunk <- pous_dat_chunk[index < 3]
# change datetime of missing obs to 10 minutes ahead of where they are
# actually recorded in time series
pous_dat_chunk[, datetime := fifelse(
index == 2,
recorded_date_time + minutes(10),
recorded_date_time
)]
# change the datetime to correct class
pous_dat_chunk[, datetime := lubridate::as_datetime(datetime)]
glimpse(pous_dat_chunk)
# need to finagle getting the 10 minute floor of dates, which is a little
# tricky - using these next two code snippets from Matt
# first create a floor date deconstructed
pous_dat_chunk[, `:=` (
year = lubridate::year(datetime),
month = lubridate::month(datetime),
day = lubridate::day(datetime),
hour = lubridate::hour(datetime),
minute = 10 * floor(lubridate::minute(datetime) / 10)
)]
# then reconstruct it
pous_dat_chunk[, date := lubridate::ymd_hm(sprintf(
"%s-%s-%s %s:%s",
year,
month,
day,
hour,
minute))]
# set the order of the datatable bc in ten minute periods with more than one
# observation, we're going to keep the last one only
setorder(
pous_dat_chunk,
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
datetime
)
# and pick out the last observation
pous_dat_chunk <-
pous_dat_chunk[, .SD[.N], by = .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
date)]
# delete old unnecessary cols
pous_dat_chunk[,
c("datetime",
"hour",
"day",
"month",
"index") := NULL]
# now expand out the dates that are in the data.table to make a complete
# list of ten-minute intervals, from the first to last observation for each
# city-utility in the data
all_dates_in_data <-
pous_dat_chunk[, .(date = seq(min(date),
max(date), by = '10 mins')),
by =  c("clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name")]
# join the power outage data back to that date backbone, rolling to fill in
pous_dat_chunk <-
pous_dat_chunk[all_dates_in_data, on = .(clean_state_name,
clean_county_name,
city_name,
utility_name,
fips,
date), roll = TRUE]
glimpse(pous_dat_chunk)
# replace -99 marker with NA
pous_dat_chunk[, customers_out_api_on :=
ifelse(customers_out == -99, NA, customers_out)]
# need to join to original backbone now to get all time we're supposed to have # flag - slow
pous_dat_chunk <- pous_dat_chunk[city_utility_time_series, on = c(
'clean_state_name',
"clean_county_name",
"fips",
"city_name",
"utility_name",
"date"
)]
pous_dat_chunk[, year := lubridate::year(date)]
# add locf by city-utility, max 4 hours
pous_dat_chunk[, new_locf_rep := na_locf(
customers_out_api_on,
option = 'locf',
na_remaining = 'keep',
maxgap = max_nas_to_impute
), by = c("clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name")]
pous_dat_chunk[, c("minute", "recorded_date_time") := NULL]
glimpse(pous_dat_chunk)
start = Sys.time()
# separate chunk
pous_dat_chunk <- pous_data[five_digit_fips %in% pous_l_split[[i]]]
pous_dat_chunk <- pous_dat_chunk[, year := lubridate::year(recorded_date_time)]
pous_dat_chunk <- pous_dat_chunk %>% rename(fips = five_digit_fips)
#pous_dat_chunk <- pous_dat_chunk[year == 2018]
# section 1: create timeseries  -------------------------------------------
# make id for city-utility-year
make_id <- unique(
pous_dat_chunk[, .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name)],
by = c(
"clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name"
)
)
# make year data.table
years <- data.table(year = c(2017, 2018, 2019, 2020))
# expand this to a datatable containing one entry for every city-utility
# and year, so four entries per city-utility
make_id <-
make_id[, .(year = years$year), by = .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name)]
# add unique ID
make_id <- make_id[, city_utility_name_id := 1:.N]
# expand the datatable into a timeseries of intervals for every year - SLOW -----
city_utility_time_series <-
make_id[, .(date = intervals_dt$date), by = .(
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
city_utility_name_id
)]
# order the pous data.table so we can correctly identify missing
# observation indicators
pous_dat_chunk <- pous_dat_chunk[order(
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
recorded_date_time,
-customers_out
)]
# index observations to identify matching time stamps which are missing ob ind
pous_dat_chunk[, index := 1:.N, by = c(
"clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name",
"recorded_date_time"
)]
# replace missing indicators with a more obvious missingness indicator, NA,
# and move them to the right spot in the time series
pous_dat_chunk[index == 2, customers_out := -99]
# remove duplicate rows (those are errors)
pous_dat_chunk <- pous_dat_chunk[index < 3]
# change datetime of missing obs to 10 minutes ahead of where they are
# actually recorded in time series
pous_dat_chunk[, datetime := fifelse(
index == 2,
recorded_date_time + minutes(10),
recorded_date_time
)]
# change the datetime to correct class
pous_dat_chunk[, datetime := lubridate::as_datetime(datetime)]
# need to finagle getting the 10 minute floor of dates, which is a little
# tricky - using these next two code snippets from Matt
# first create a floor date deconstructed
pous_dat_chunk[, `:=` (
year = lubridate::year(datetime),
month = lubridate::month(datetime),
day = lubridate::day(datetime),
hour = lubridate::hour(datetime),
minute = 10 * floor(lubridate::minute(datetime) / 10)
)]
# then reconstruct it
pous_dat_chunk[, date := lubridate::ymd_hm(sprintf(
"%s-%s-%s %s:%s",
year,
month,
day,
hour,
minute))]
# set the order of the datatable bc in ten minute periods with more than one
# observation, we're going to keep the last one only
setorder(
pous_dat_chunk,
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
datetime
)
# and pick out the last observation
pous_dat_chunk <-
pous_dat_chunk[, .SD[.N], by = .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
date)]
# delete old unnecessary cols
pous_dat_chunk[,
c("datetime",
"hour",
"day",
"month",
"index") := NULL]
# now expand out the dates that are in the data.table to make a complete
# list of ten-minute intervals, from the first to last observation for each
# city-utility in the data
all_dates_in_data <-
pous_dat_chunk[, .(date = seq(min(date),
max(date), by = '10 mins')),
by =  c("clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name")]
# join the power outage data back to that date backbone, rolling to fill in
pous_dat_chunk <-
pous_dat_chunk[all_dates_in_data, on = .(clean_state_name,
clean_county_name,
city_name,
utility_name,
fips,
date), roll = TRUE]
# replace -99 marker with NA
pous_dat_chunk[, customers_out_api_on :=
ifelse(customers_out == -99, NA, customers_out)]
# need to join to original backbone now to get all time we're supposed to have # flag - slow
pous_dat_chunk <- pous_dat_chunk[city_utility_time_series, on = c(
'clean_state_name',
"clean_county_name",
"fips",
"city_name",
"utility_name",
"date"
)]
pous_dat_chunk[, year := lubridate::year(date)]
glimpse(pous_dat_chunk)
# add locf by city-utility, max 4 hours
pous_dat_chunk[, new_locf_rep := na_locf(
customers_out_api_on,
option = 'locf',
na_remaining = 'keep',
maxgap = max_nas_to_impute
), by = c("clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name")]
pous_dat_chunk[, c("minute", "recorded_date_time") := NULL]
pous_dat_chunk <- pous_data[five_digit_fips %in% pous_l_split[[i]]]
pous_dat_chunk <- pous_dat_chunk[, year := lubridate::year(recorded_date_time)]
pous_dat_chunk <- pous_dat_chunk %>% rename(fips = five_digit_fips)
#pous_dat_chunk <- pous_dat_chunk[year == 2018]
# section 1: create timeseries  -------------------------------------------
# make id for city-utility-year
make_id <- unique(
pous_dat_chunk[, .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name)],
by = c(
"clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name"
)
)
# make year data.table
years <- data.table(year = c(2017, 2018, 2019, 2020))
# expand this to a datatable containing one entry for every city-utility
# and year, so four entries per city-utility
make_id <-
make_id[, .(year = years$year), by = .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name)]
# add unique ID
make_id <- make_id[, city_utility_name_id := 1:.N]
# expand the datatable into a timeseries of intervals for every year - SLOW -----
city_utility_time_series <-
make_id[, .(date = intervals_dt$date), by = .(
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
city_utility_name_id
)]
# order the pous data.table so we can correctly identify missing
# observation indicators
pous_dat_chunk <- pous_dat_chunk[order(
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
recorded_date_time,
-customers_out
)]
# index observations to identify matching time stamps which are missing ob ind
pous_dat_chunk[, index := 1:.N, by = c(
"clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name",
"recorded_date_time"
)]
# replace missing indicators with a more obvious missingness indicator, NA,
# and move them to the right spot in the time series
pous_dat_chunk[index == 2, customers_out := -99]
# remove duplicate rows (those are errors)
pous_dat_chunk <- pous_dat_chunk[index < 3]
# change datetime of missing obs to 10 minutes ahead of where they are
# actually recorded in time series
pous_dat_chunk[, datetime := fifelse(
index == 2,
recorded_date_time + minutes(10),
recorded_date_time
)]
# change the datetime to correct class
pous_dat_chunk[, datetime := lubridate::as_datetime(datetime)]
# need to finagle getting the 10 minute floor of dates, which is a little
# tricky - using these next two code snippets from Matt
# first create a floor date deconstructed
pous_dat_chunk[, `:=` (
year = lubridate::year(datetime),
month = lubridate::month(datetime),
day = lubridate::day(datetime),
hour = lubridate::hour(datetime),
minute = 10 * floor(lubridate::minute(datetime) / 10)
)]
# then reconstruct it
pous_dat_chunk[, date := lubridate::ymd_hm(sprintf(
"%s-%s-%s %s:%s",
year,
month,
day,
hour,
minute))]
# set the order of the datatable bc in ten minute periods with more than one
# observation, we're going to keep the last one only
setorder(
pous_dat_chunk,
clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
datetime
)
# and pick out the last observation
pous_dat_chunk <-
pous_dat_chunk[, .SD[.N], by = .(clean_state_name,
clean_county_name,
fips,
city_name,
utility_name,
date)]
# delete old unnecessary cols
pous_dat_chunk[,
c("datetime",
"hour",
"day",
"month",
"index") := NULL]
# now expand out the dates that are in the data.table to make a complete
# list of ten-minute intervals, from the first to last observation for each
# city-utility in the data
all_dates_in_data <-
pous_dat_chunk[, .(date = seq(min(date),
max(date), by = '10 mins')),
by =  c("clean_state_name",
"clean_county_name",
"fips",
"city_name",
"utility_name")]
# join the power outage data back to that date backbone, rolling to fill in
pous_dat_chunk <-
pous_dat_chunk[all_dates_in_data, on = .(clean_state_name,
clean_county_name,
city_name,
utility_name,
fips,
date), roll = TRUE]
# replace -99 marker with NA
pous_dat_chunk[, customers_out_api_on :=
ifelse(customers_out == -99, NA, customers_out)]
# need to join to original backbone now to get all time we're supposed to have # flag - slow
pous_dat_chunk <- pous_dat_chunk[city_utility_time_series, on = c(
'clean_state_name',
"clean_county_name",
"fips",
"city_name",
"utility_name",
"date"
)]
pous_dat_chunk[, year := lubridate::year(date)]
ii <- pous_dat_chunk %>% group_by(city_utility_name_id) %>% summarize(n = sum(!is.na(customers_out_api_on)))
View(ii)
