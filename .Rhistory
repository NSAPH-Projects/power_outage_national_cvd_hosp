source_and_clean(script)
}
script = scripts[[1]]
script = scripts[[2]]
script
source_and_clean(script)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/alt_run_all.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b05_identify_daily_binary_exposure.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b06_identify_outages_percentile.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b07_create_exposure_data_for_upload.R", echo=TRUE)
# stop timing
stop_time <- Sys.time()
log_info(paste0('Total run time was ', round(
difftime(stop_time, start_time, units = "mins"), 2
), ' minutes'))
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a00_define_contiguous_US.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a01_find_eia_state_customers.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a02_get_county_census_cust_est.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a03_county_customer_census_estimates.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a02_get_county_census_cust_est.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a03_county_customer_census_estimates.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a01_find_eia_state_customers.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a02_get_county_census_cust_est.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a03_county_customer_census_estimates.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a03_county_customer_census_estimates.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b01_read_and_clean.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b03_attach_denoms.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b04_id_outages_continuous_measures.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b05_identify_daily_binary_exposure.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b06_identify_outages_percentile.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b07_create_exposure_data_for_upload.R", echo=TRUE)
write_fst(hrs_out, here(
'data_for_upload',
'power_outage_exposure_data_cleaning_output',
'daily_hrs_out_oct_31.fst'
))
pacman::p_load(tidyverse, here, lubridate, data.table, fst)
source(here('code', 'functions', 'expand_to_hourly_helpers.R'))
counties <-
read_fst(here("local_data",
"power_outage_exposure_data_cleaning_output",
"hourly_data_with_coverage_exclusions.fst")) |>
as.data.table()
counties <-
counties[, .(
clean_state_name,
clean_county_name,
five_digit_fips,
year,
hour,
customers_out_hourly = customers_out_hourly_locf,
customers_served_total = downscaled_county_estimate
)]
# identify continuous hours out per day for different cut points
counties <- identify_power_outage_on_all_counties(counties, cut_point = 0.005)
counties <- identify_power_outage_on_all_counties(counties, cut_point = 0.01)
counties <- identify_power_outage_on_all_counties(counties, cut_point = 0.03)
counties <- identify_power_outage_on_all_counties(counties, cut_point = 0.05)
setDT(counties)
counties[, day := lubridate::floor_date(hour, unit = 'day')]
hrs_out <- counties[, .(
n_hrs_out_0.005 = sum(po_on_0.005, na.rm = TRUE),
n_hrs_out_0.01 = sum(po_on_0.01, na.rm = TRUE),
n_hrs_out_0.03 = sum(po_on_0.03, na.rm = TRUE),
n_hrs_out_0.05 = sum(po_on_0.05, na.rm = TRUE)
), by = .(five_digit_fips, day)]
write_fst(hrs_out, here(
'data_for_upload',
'power_outage_exposure_data_cleaning_output',
'daily_hrs_out_oct_31.fst'
))
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b05_identify_daily_binary_exposure.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b07_create_exposure_data_for_upload.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b07_create_exposure_data_for_upload.R", echo=TRUE)
View(all_exposures)
length(all_exposure$five_digit_fips)
length(all_exposures$five_digit_fips)
length(unique(all_exposures$five_digit_fips))
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/00_run_all.R", echo=TRUE)
# This script creates files that have
# a) a crosswalk of state fips codes, county fips codes, state abbreviations,
# and state names for counties we wish to capture in the POUS dataset because
# they are in the contiguous US
# b) a list of counties and five-digit county fips codes that are in the
# contiguous US
# c) a shapefile of county boundaries for those counties in the contiguous US
# that we're aiming to include
# Last updated: Oct 3rd, 2024
# Author: Heather
pacman::p_load(sf, here, tidyverse, data.table, arrow)
# set shapefile for whole project
# states we wish to include: just being very clear about this list
cotus_state_fips_abbrev <- data.frame(
state_fips =
c("01", "04", "05", "06", "08", "09", "10", "12", "13", "16", "17", "18",
"19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30",
"31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42",
"44", "45", "46", "47", "48", "49", "50", "51", "53", "54", "55", "56",
"02", "15"),
state_ab =
c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "ID", "IL", "IN",
"IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT",
"NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA",
"RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY",
"AK", "HI"),
state_name =
c("Alabama", "Arizona", "Arkansas", "California", "Colorado", "Connecticut",
"Delaware", "Florida", "Georgia", "Idaho", "Illinois", "Indiana", "Iowa",
"Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts",
"Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska",
"Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York",
"North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
"Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
"Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming",
"Alaska", "Hawaii"),
stringsAsFactors = FALSE
)
write_rds(cotus_state_fips_abbrev, here("data_for_upload", 'cotus_state_fips_abbrev.RDS'))
# get and filter shapefile to that set of counties
county_shp <- tigris::counties(year = 2020) # get
pacman::p_load(sf, here, tidyverse, data.table, arrow)
# set shapefile for whole project
# states we wish to include: just being very clear about this list
cotus_state_fips_abbrev <- data.frame(
state_fips =
c("01", "04", "05", "06", "08", "09", "10", "12", "13", "16", "17", "18",
"19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30",
"31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42",
"44", "45", "46", "47", "48", "49", "50", "51", "53", "54", "55", "56",
"02", "15"),
state_ab =
c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "ID", "IL", "IN",
"IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT",
"NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA",
"RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY",
"AK", "HI"),
state_name =
c("Alabama", "Arizona", "Arkansas", "California", "Colorado", "Connecticut",
"Delaware", "Florida", "Georgia", "Idaho", "Illinois", "Indiana", "Iowa",
"Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts",
"Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska",
"Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York",
"North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
"Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
"Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming",
"Alaska", "Hawaii"),
stringsAsFactors = FALSE
)
write_rds(cotus_state_fips_abbrev, here("data_for_upload", 'cotus_state_fips_abbrev.RDS'))
# get and filter shapefile to that set of counties
county_shp <- tigris::counties(year = 2020) # get
county_shp <- county_shp |> # save relevant info only
mutate(five_digit_fips = paste0(STATEFP, COUNTYFP)) |>
select(five_digit_fips,
state_fips = STATEFP,
county_fips = COUNTYFP,
county_name = NAME)
county_shp <- county_shp |>
filter(state_fips %in% cotus_state_fips_abbrev$state_fips)
# reproject to albers
epsg_code <- 5070
county_shp <- st_transform(county_shp, crs = epsg_code)
# write
write_parquet(county_shp,
here("data_for_upload", "cotus_county_shp_w_fips.parquet"))
# write
st_write(sf_data, "output_file.gpkg", driver = "GPKG")
# write
st_write(county_shp, here("data_for_upload", "cotus_county_shp_w_fips.gpkg"), driver = "GPKG")
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a00_define_contiguous_US.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/00_run_all.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/00_run_all.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/00_run_all.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/00_run_all.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b05_identify_daily_binary_exposure.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b06_identify_outages_percentile.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b07_create_exposure_data_for_upload.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b03_attach_denoms.R", echo=TRUE)
temp_data <-
rbindlist(lapply(FUN = read_rds, X = list.files(
here("local_data", "effect_mod_data", "temp_data"),
full.names = T
)))
pacman::p_load(here, data.table)
temp_data <-
rbindlist(lapply(FUN = read_rds, X = list.files(
here("local_data", "effect_mod_data", "temp_data"),
full.names = T
)))
pacman::p_load(here, data.table, tidyverse)
temp_data <-
rbindlist(lapply(FUN = read_rds, X = list.files(
here("local_data", "effect_mod_data", "temp_data"),
full.names = T
)))
# get dates in correct class, and make a week number, but also if the year has
# more than 52 weeks just include those days in the 52nd week
temp_data  <- temp_data  %>%
select(date, five_digit_fips = fips, tmean) %>%
mutate(date = lubridate::dmy(date),
week_num = lubridate::epiweek(date)) %>%
mutate(week_num = ifelse(week_num == 53, 52, week_num))
# split into years we use for establishing baseline, and years we want to ID
temp_data_1981_2010 <-
temp_data %>%
filter(date > "1980-12-30" & date < "2011-01-01")
temp_data_2018_2020 <-
temp_data %>%
filter(date > "2017-12-30") %>%
select(five_digit_fips, date, tmean)
percentiles <- temp_data_1981_2010 %>%
group_by(five_digit_fips, week_num) %>%
summarise(hot_85th = quantile(tmean, probs = 0.85),
cold_15th = quantile(tmean, probs = 0.15))
i <-percentiles %>% group_by(five_digit_fips) %>% summarize(n = n())
View(i)
View(percentiles)
days_over_thresholds <- temp_data_2018_2020 %>%
mutate(over_24 = ifelse(tmean > 24, 1, 0),
under_0 = ifelse(tmean < 0, 1, 0))
# join threshold info to days in 2018-2020
days_over_thresholds <-
days_over_thresholds %>%
left_join(percentiles)
days_over_thresholds <- temp_data_2018_2020 %>%
mutate(over_24 = ifelse(tmean > 24, 1, 0),
under_0 = ifelse(tmean < 0, 1, 0))
glimpse(percentiles)
i <- days_over_thresholds %>% group_by(five_digit_fips) %>% summarize(n = n())
View(i)
glimpse(days_over_thresholds)
glimpse(percentiles)
temp_data <-
rbindlist(lapply(FUN = read_rds, X = list.files(
here("local_data", "effect_mod_data", "temp_data"),
full.names = T
)))
# Do ----------------------------------------------------------------------
# get dates in correct class, and make a week number, but also if the year has
# more than 52 weeks just include those days in the 52nd week
temp_data  <- temp_data  %>%
select(date, five_digit_fips = fips, tmean) %>%
mutate(date = lubridate::dmy(date),
week_num = lubridate::epiweek(date)) %>%
mutate(week_num = ifelse(week_num == 53, 52, week_num))
# split into years we use for establishing baseline, and years we want to ID
temp_data_1981_2010 <-
temp_data %>%
filter(date > "1980-12-30" & date < "2011-01-01")
temp_data_2018_2020 <-
temp_data %>%
filter(date > "2017-12-30") %>%
select(five_digit_fips, date, week_num, tmean)
# group the baseline years by week and county, and calculate 85th and 15th
# percentiles using weekly long-term average to establish cutoffs for anom hot
# and cold days by county and week
percentiles <- temp_data_1981_2010 %>%
group_by(five_digit_fips, week_num) %>%
summarise(hot_85th = quantile(tmean, probs = 0.85),
cold_15th = quantile(tmean, probs = 0.15))
# group the years we want to id by week and county, and find days that were
# over the threshold temps (over a threshold (>24°C) and under (<0°C))
# note from alex: changed this threshold to account for power lines:
# https://ieeexplore.ieee.org/abstract/document/9123900
# since using tmean, this likely includes freezing temperature during some
# point of the day
days_over_thresholds <- temp_data_2018_2020 %>%
mutate(over_24 = ifelse(tmean > 24, 1, 0),
under_0 = ifelse(tmean < 0, 1, 0))
glimpse(days_over_thresholds)
# join threshold info to days in 2018-2020
days_over_thresholds <-
days_over_thresholds %>%
left_join(percentiles)
# make a new col to see which days meet both criteria to be anom hot or cold
days_over_thresholds <- days_over_thresholds %>%
mutate(anomhot = ifelse(over_24 == 1 & tmean >= hot_85th, 1, 0),
anomcold = ifelse(under_0 == 1 & tmean <= cold_15th, 1,0))
View(days_over_thresholds)
write_rds(
days_over_thresholds,
here(
'data_for_upload',
'effect_mod_data',
'hot_and_cold_days.RDS'
)
)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/effect_mod_data_prep/03_clean_temp.R", echo=TRUE)
acs_dat <- read_csv(here(
'local_data',
'effect_mod',
'nhgis0011_ds244_20195_county.csv'
))
pacman::p_load(tidyverse, here)
acs_dat <- read_csv(here(
'local_data',
'effect_mod',
'nhgis0011_ds244_20195_county.csv'
))
acs_dat <- read_csv(here(
'local_data',
'effect_mod',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
here()
acs_dat <- read_csv(here(
'local_data',
'effect_mod_data',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
View(acs_data)
View(acs_dat)
# Load and clean the ACS data that we'll use as poverty measures.
# Author: Heather
# Date: Nov. 6th, 2024
# Libraires ---------------------------------------------------------------
pacman::p_load(tidyverse, here)
source(
here(
'code',
'data_prep',
'effect_mod_data_prep',
'effect_mod_data_cleaning_helpers.R'
)
)
acs_dat <- read_csv(here(
'local_data',
'effect_mod_data',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
acs_dat <- acs_dat %>%
mutate(total_below_pov = sum(ALWVE002 + ALWVE003),
p_below_pov = total_below_pov / ALWVE001)
glimpse(acs_dat)
acs_dat <- acs_dat %>% mutate(pov_quartile = find_quartiles(p_below_pov))
View(acs_dat)
# Load and clean the ACS data that we'll use as poverty measures.
# Author: Heather
# Date: Nov. 6th, 2024
# Libraires ---------------------------------------------------------------
pacman::p_load(tidyverse, here)
source(
here(
'code',
'data_prep',
'effect_mod_data_prep',
'effect_mod_data_cleaning_helpers.R'
)
)
# Read --------------------------------------------------------------------
acs_dat <- read_csv(here(
'local_data',
'effect_mod_data',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
# going to calculate total number of people making below poverty line
# as a percentage of total county pop
# calculate quartiles of that
# and then compare 1 to 4th quartile
acs_dat <- acs_dat %>%
mutate(total_below_pov = sum(ALWVE002 + ALWVE003),
p_below_pov = total_below_pov / ALWVE001,
five_digit_fips = paste0(STATEA, COUNTYA)) %>%
select(five_digit_fips, total_below_pov, p_below_pov)
acs_dat <- acs_dat %>% mutate(pov_quartile = find_quartiles(p_below_pov))
View(acs_dat)
hist(acs_dat$pov_quartile)
length(unique(acs_dat$five_digit_fips))
acs_dat <- acs_dat %>% filter(five_digit_fips %in% fips$five_digit_fips)
# Load and clean the ACS data that we'll use as poverty measures.
# Author: Heather
# Date: Nov. 6th, 2024
# Libraires ---------------------------------------------------------------
pacman::p_load(tidyverse, here)
source(
here(
'code',
'data_prep',
'effect_mod_data_prep',
'effect_mod_data_cleaning_helpers.R'
)
)
# Read --------------------------------------------------------------------
fips <- read_rds(here('data_for_upload', 'cotus_county_list_of_fips.RDS'))
acs_dat <- read_csv(here(
'local_data',
'effect_mod_data',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
# going to calculate total number of people making below poverty line
# as a percentage of total county pop
# calculate quartiles of that
# and then compare 1 to 4th quartile
acs_dat <- acs_dat %>%
mutate(total_below_pov = sum(ALWVE002 + ALWVE003),
p_below_pov = total_below_pov / ALWVE001,
five_digit_fips = paste0(STATEA, COUNTYA)) %>%
select(five_digit_fips, total_below_pov, p_below_pov)
acs_dat <-
acs_dat %>%
mutate(pov_quartile = find_quartiles(p_below_pov))
acs_dat <- acs_dat %>% filter(five_digit_fips %in% fips$five_digit_fips)
write_rds(acs_dat,
here('data_for_upload', 'effect_mod_data', 'pov_measures.RDS'))
View(acs_dat)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/poverty_measures.R", echo=TRUE)
View(acs_dat)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/effect_mod_data_prep/03_clean_temp.R", echo=TRUE)
pacman::p_load(here, tidyverse, data.table, fst)
write_fst(
days_over_thresholds,
here(
'data_for_upload',
'effect_mod_data',
'hot_and_cold_days.fst'
)
)
pacman::p_load(tidyverse, readxl, here)
# read customer counts
# list eia dataset names - these are datasets containing electrical customer
# counts by state
eia_sets <-
list.files(
here(
"data_for_upload",
"power_outage_exposure_data_cleaning_raw_data",
"EIA"
),
full.names = TRUE
)
View(eia_sets)
# read into a list for easy processing
cust_counts <-
lapply(eia_sets,
read_excel,
skip = 3,
col_names = F)
View(cust_counts)
View(cust_counts[[1]])
pacman::p_load(tidyverse, here, janitor, tidytext, snakecase, readxl,
data.table, fst)
# raw data from POUS
raw_pous_read <-
fread(
here(
"local_data",
"power_outage_exposure_data_cleaning_raw_data",
"POUS_Export_Raw_CityByUtility_20170101_20201231.csv"
)
)#, n_max = 10000)
glimpse(raw_pous_read)
o <- raw_pous_read %>% filter(StateName == 'PuertoRico')
sort(unique(raw_pous_read$StateName))
# Print the summary table
print(summary_table)
source("~/.active-rstudio-document", echo=TRUE)
library(dplyr)
library(tidyr)
# Example dataset
data <- data.frame(
five_digit_fips = sample(10000:99999, 1000, replace = TRUE),
day = sample(seq.Date(as.Date('2020-01-01'), as.Date('2020-12-31'), by="day"), 1000, replace = TRUE),
exposed_8_hrs_0.01 = runif(1000, 0, 1),
max_temp = runif(1000, 50, 100),
precip = runif(1000, 0, 10),
wind_speed = runif(1000, 0, 20),
n_benes_older_75 = sample(0:1, 1000, replace = TRUE),
dme_quartile = sample(1:4, 1000, replace = TRUE),
pov_quartile = sample(1:4, 1000, replace = TRUE)
)
# Function to calculate quartiles
calculate_quartiles <- function(data, group_var, exposure_var) {
data %>%
group_by(!!sym(group_var)) %>%
summarize(
Q1 = quantile(!!sym(exposure_var), 0.25, na.rm = TRUE),
Q2 = quantile(!!sym(exposure_var), 0.50, na.rm = TRUE),
Q3 = quantile(!!sym(exposure_var), 0.75, na.rm = TRUE),
Q4 = quantile(!!sym(exposure_var), 1.00, na.rm = TRUE)
) %>%
ungroup()
}
# Calculate quartiles for the entire dataset
overall_quartiles <- calculate_quartiles(data, "five_digit_fips", "exposed_8_hrs_0.01")
# Calculate quartiles for each subgroup
age_quartiles <- calculate_quartiles(data, "n_benes_older_75", "exposed_8_hrs_0.01")
dme_quartiles <- calculate_quartiles(data, "dme_quartile", "exposed_8_hrs_0.01")
pov_quartiles <- calculate_quartiles(data, "pov_quartile", "exposed_8_hrs_0.01")
wind_quartiles <- calculate_quartiles(data, "wind_speed", "exposed_8_hrs_0.01")
precip_quartiles <- calculate_quartiles(data, "precip", "exposed_8_hrs_0.01")
temp_quartiles <- calculate_quartiles(data, "max_temp", "exposed_8_hrs_0.01")
# Combine all quartiles into a single table
summary_table <- bind_rows(
overall_quartiles %>% mutate(Group = "Overall"),
age_quartiles %>% mutate(Group = ifelse(n_benes_older_75 == 1, "Age >= 75", "Age < 75")),
dme_quartiles %>% mutate(Group = paste("DME Quartile", dme_quartile)),
pov_quartiles %>% mutate(Group = paste("Poverty Quartile", pov_quartile)),
wind_quartiles %>% mutate(Group = "Wind Speed Quartiles"),
precip_quartiles %>% mutate(Group = "Precipitation Quartiles"),
temp_quartiles %>% mutate(Group = "Temperature Quartiles")
) %>%
select(Group, Q1, Q2, Q3, Q4)
# Print the summary table
print(summary_table)
View(summary_table)
