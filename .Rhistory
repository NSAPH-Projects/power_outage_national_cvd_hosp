all_exposures,
here(
'data',
"power_outage_exposure_data_cleaning_output",
"analytic_exposure_data_2018.parquet"
)
)
renv::remove(MASS)
renv::remove('MASS')
renv::snapshot()
renv::update()
pacman::p_load(arrow, tidyverse, here, sf, ggthemes, arrow)
meteo_vars <- read_parquet(here('data', 'meteo_vars.parquet'))
county_backbone <- read_rds(here("data", "cotus_county_shp_w_fips.RDS"))
us_counties <- readRDS(here("data", "counties_sf.RDS")) %>%
rename(five_digit_fips = county_fips) %>%
filter(five_digit_fips %in% county_backbone$five_digit_fips)
meteo_vars <- meteo_vars %>%
group_by(five_digit_fips) %>%
summarize(
mean_min_temp = mean(min_temp),
mean_max_temp = mean(max_temp),
max_precip = max(precip),
max_wind = max(wind_speed)
)
county_backbone <- county_backbone %>% left_join(meteo_vars)
p1 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = mean_min_temp, color = NA)) +
scale_fill_viridis_c(name = "Mean minimum temperature gridmet for 2018") +
theme_map() +
ggtitle(paste0('Mean minimum temp'))
ggsave(p1, here('figures', 'figures_output', 'plot_min_temp.png'))
ggsave(plot = p1, here('figures', 'figures_output', 'plot_min_temp.png'))
p1 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = mean_min_temp), color = NA) +
scale_fill_viridis_c(name = "Mean minimum temperature gridmet for 2018") +
theme_map() +
ggtitle(paste0('Mean minimum temp'))
ggsave(plot = p1, here('figures', 'figures_output', 'plot_min_temp.png'))
pdf(here("figures", "figures_output", "weather_vars.pdf"))
p1 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = mean_min_temp), color = NA) +
scale_fill_viridis_c(name = "Mean minimum temperature gridmet for 2018") +
theme_map() +
ggtitle(paste0('Mean minimum temp'))
print(p1)
p2 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = mean_max_temp, color = NA)) +
scale_fill_viridis_c(name = "Mean maximum temperature gridmet for 2018") +
theme_map() +
ggtitle(paste0('Mean maximum temp'))
print(p2)
p3 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = max_precip, color = NA)) +
scale_fill_viridis_c(name = "Maximum daily precipitation gridmet for 2018") +
theme_map() +
ggtitle(paste0('Max precip'))
print(p3)
p3 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = max_precip, color = NA)) +
scale_fill_viridis_c(name = "Maximum wind speed gridmet for 2018") +
theme_map() +
ggtitle(paste0('Max wind speed'))
print(p4)
p4 <- county_backbone |>
geom_sf(aes(fill = max_precip) +
theme_map() +
p4 <- county_backbone |>
)
p4 <- county_backbone |>
p4 <- county_backbone %>%
)
p4 <- county_backbone %>%
)
p4 <- county_backbone %>%
ggplot() +
geom_sf(aes(fill = max_precip) +
scale_fill_viridis_c(name = "Maximum wind speed gridmet for 2018") +
theme_map() +
ggtitle(paste0('Max wind speed'))
p4 <- county_backbone %>%
p4 <- county_backbone %>%
ggplot() +
geom_sf(aes(fill = max_precip)) +
scale_fill_viridis_c(name = "Maximum wind speed gridmet for 2018") +
theme_map() +
ggtitle(paste0('Max wind speed'))
print(p4)
dev.off()
# Plot gridmet data already cleaned by NSAPH
# Libraries ---------------------------------------------------------------
pacman::p_load(arrow, tidyverse, here, sf, ggthemes, arrow)
# Read --------------------------------------------------------------------
meteo_vars <- read_parquet(here('data', 'meteo_vars.parquet'))
county_backbone <- read_rds(here("data", "cotus_county_shp_w_fips.RDS"))
us_counties <- readRDS(here("data", "counties_sf.RDS")) %>%
rename(five_digit_fips = county_fips) %>%
filter(five_digit_fips %in% county_backbone$five_digit_fips)
pacman::p_load(arrow, tidyverse, here, sf, ggthemes, arrow)
meteo_vars <- read_parquet(here('data', 'meteo_vars.parquet'))
county_backbone <- read_rds(here("data", "cotus_county_shp_w_fips.RDS"))
meteo_vars <- meteo_vars %>%
group_by(five_digit_fips) %>%
summarize(
mean_min_temp = mean(min_temp),
mean_max_temp = mean(max_temp),
max_precip = max(precip),
max_wind = max(wind_speed)
)
county_backbone <- county_backbone %>% left_join(meteo_vars)
pdf(here("figures", "figures_output", "weather_vars.pdf"))
p1 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = mean_min_temp), color = NA) +
scale_fill_viridis_c(name = "Mean minimum temperature gridmet for 2018") +
theme_map() +
ggtitle(paste0('Mean minimum temp'))
print(p1)
p2 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = mean_max_temp)) +
scale_fill_viridis_c(name = "Mean maximum temperature gridmet for 2018") +
theme_map() +
ggtitle(paste0('Mean maximum temp'))
print(p2)
p3 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = max_precip)) +
scale_fill_viridis_c(name = "Maximum daily precipitation gridmet for 2018") +
theme_map() +
ggtitle(paste0('Max precip'))
print(p3)
p4 <- county_backbone %>%
ggplot() +
geom_sf(aes(fill = max_precip)) +
scale_fill_viridis_c(name = "Maximum wind speed gridmet for 2018") +
theme_map() +
ggtitle(paste0('Max wind speed'))
print(p4)
dev.off()
# Plot gridmet data already cleaned by NSAPH
# Libraries ---------------------------------------------------------------
pacman::p_load(arrow, tidyverse, here, sf, ggthemes, arrow)
# Read --------------------------------------------------------------------
meteo_vars <- read_parquet(here('data', 'meteo_vars.parquet'))
county_backbone <- read_rds(here("data", "cotus_county_shp_w_fips.RDS"))
# Plot real quick ---------------------------------------------------------
meteo_vars <- meteo_vars %>%
group_by(five_digit_fips) %>%
summarize(
mean_min_temp = mean(min_temp),
mean_max_temp = mean(max_temp),
max_precip = max(precip),
max_wind = max(wind_speed)
)
county_backbone <- county_backbone %>% left_join(meteo_vars)
pdf(here("figures", "figures_output", "weather_vars.pdf"))
p1 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = mean_min_temp), color = NA) +
scale_fill_viridis_c(name = "Mean minimum temperature gridmet for 2018") +
theme_map() +
ggtitle(paste0('Mean minimum temp'))
#ggsave(plot = p1, here('figures', 'figures_output', 'plot_min_temp.png'))
print(p1)
p2 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = mean_max_temp), color = NA) +
scale_fill_viridis_c(name = "Mean maximum temperature gridmet for 2018") +
theme_map() +
ggtitle(paste0('Mean maximum temp'))
print(p2)
p3 <- county_backbone |>
ggplot() +
geom_sf(aes(fill = max_precip), color = NA) +
scale_fill_viridis_c(name = "Maximum daily precipitation gridmet for 2018") +
theme_map() +
ggtitle(paste0('Max precip'))
print(p3)
p4 <- county_backbone %>%
ggplot() +
geom_sf(aes(fill = max_precip), color = NA) +
scale_fill_viridis_c(name = "Maximum wind speed gridmet for 2018") +
theme_map() +
ggtitle(paste0('Max wind speed'))
print(p4)
dev.off()
pacman::p_load(arrow, data.table, tidyverse, here, fst)
outages <- read_parquet(
here(
'data',
'power_outage_exposure_data_cleaning_output',
'analytic_exposure_data_all_years.parquet'
)
)
denoms <- read_fst(
here(
'data',
'power_outage_exposure_data_cleaning_output',
"county_customer_denoms_and_p_missing.fst"
)
)
outages <- outages %>% left_join(denoms)
outages <- outages %>% filter(percent_served > 0.5 & county_customers > 1000)
p1 <-
outages %>%
ggplot() +
geom_point(aes(x = day, y = n_hrs_out_0.01), alpha = 0.2)
ggsave(p1, filename= here('figures', 'figures_output', 'outages_by_day.png'))
p1 <-
outages %>%
ggplot() +
geom_point(aes(x = day, y = n_hrs_out_0.01), alpha = 0.01)
ggsave(p1, filename= here('figures', 'figures_output', 'outages_by_day.png'))
p1 <-
outages %>%
ggplot() +
geom_point(aes(x = day, y = n_hrs_out_0.01), alpha = 0.01, size = 5)
ggsave(p1, filename= here('figures', 'figures_output', 'outages_by_day.png'))
7/8
pacman::p_load(arrow, data.table, tidyverse, here, fst)
outages <- read_parquet(
here(
'data',
'power_outage_exposure_data_cleaning_output',
'analytic_exposure_data_all_years.parquet'
)
)
denoms <- read_fst(
here(
'data',
'power_outage_exposure_data_cleaning_output',
"county_customer_denoms_and_p_missing.fst"
)
)
outages <- outages %>% left_join(denoms)
outages <- outages %>% filter(percent_served > 0.5 & county_customers > 1000)
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = mean(exposed_8_hrs_0.005))
View(p2)
p2 <- p2 %>%
ggplot() +
geom_point(aes(x = month, y = mean_outages)) + facet_grid( ~ year)
ggsave(p2,
filename = here('figures', 'figures_output', 'mean_outages_by_weekday.png'))
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = mean(exposed_8_hrs_0.005))
p2 <- p2 %>%
ggplot() +
geom_point(aes(x = month, y = mean_outages)) + facet_grid( ~ year)
ggsave(p2,
filename = here('figures', 'figures_output', 'mean_outages_by_weekday.png'))
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = mean(exposed_8_hrs_0.005))
p2 <- p2 %>%
ggplot() +
geom_point(aes(x = month, y = mean_outages)) + facet_grid( ~ year)
p2
p2 <- p2 %>%
ggplot() +
geom_point(aes(x = day_of_week, y = mean_outages)) + facet_grid( ~ year)
p2
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = mean(exposed_8_hrs_0.005))
p2
p2 <- p2 %>%
ggplot() +
geom_point(aes(x = day_of_week, y = mean_outages)) + facet_grid( ~ year)
p2
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = sum(exposed_8_hrs_0.005))
p2 <- p2 %>%
ggplot() +
geom_point(aes(x = day_of_week, y = mean_outages)) + facet_grid( ~ year)
p2
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = sum(exposed_8_hrs_0.005))
p2 <- p2 %>%
ggplot() +
geom_point(aes(x = day_of_week, y = mean_outages)) +
geom_jitter(alpha = 0.5, size = 3) +
facet_grid( ~ year)
p2
p2 <- p2 %>%
ggplot() +
geom_jitter(aes(x = day_of_week, y = mean_outages), alpha = 0.5, size = 3) +
facet_grid( ~ year)
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = sum(exposed_8_hrs_0.005))
p2 <- p2 %>%
ggplot() +
geom_jitter(aes(x = day_of_week, y = mean_outages), alpha = 0.5, size = 3) +
facet_grid( ~ year)
p2
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = sum(exposed_8_hrs_0.005))
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(num_outages = sum(exposed_8_hrs_0.005))
p2 <- p2 %>% group_by(day_of_week) %>% summarize(n = mean(num_outages))
View(p2)
?wday
pacman::p_load(arrow, data.table, tidyverse, here, fst)
outages <- read_parquet(
here(
'data',
'power_outage_exposure_data_cleaning_output',
'analytic_exposure_data_all_years.parquet'
)
)
denoms <- read_fst(
here(
'data',
'power_outage_exposure_data_cleaning_output',
"county_customer_denoms_and_p_missing.fst"
)
)
outages <- outages %>% left_join(denoms)
outages <- outages %>% filter(percent_served > 0.5 & county_customers > 1000)
day_of_week <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(num_outages = sum(exposed_8_hrs_0.005))
day_of_week %>%
group_by(day_of_week) %>%
summarize(n = mean(num_outages)) %>%
knitr::kable()
p2 <- day_of_week %>%
ggplot() +
geom_jitter(aes(x = day_of_week, y = mean_outages),
alpha = 0.5,
size = 3) +
facet_grid(~ year)
p2
p2 <- day_of_week %>%
ggplot() +
geom_jitter(aes(x = day_of_week, y = num_outages),
alpha = 0.5,
size = 3) +
facet_grid(~ year)
p2
p2 <- day_of_week %>%
ggplot() +
geom_jitter(aes(x = day_of_week, y = num_outages),
alpha = 0.02,
size = 3) +
facet_grid(~ year)
p2
pacman::p_load(arrow, data.table, tidyverse, here, fst)
outages <- read_parquet(
here(
'data',
'power_outage_exposure_data_cleaning_output',
'analytic_exposure_data_all_years.parquet'
)
)
min(outages$day)
max(outages$day)
i <- outages %>% filter(year == 2019)
sum(i$exposed_8_hrs_0.005)
sum(i$exposed_8_hrs_0.005)/length(i$exposed_8_hrs_0.005)
# Expand Out Power Outages
# This script expands power outage data into a time series, from its raw
# form where the dataset only includes entries for changes in customers_out
# (see the POUS documentation for an explanation of how the raw data is
# structured). It does this one county at a time, and saves each expanded county
# in the 'hourly county' folder in the 'data' folder. Need to clean up
# ordering of this script.
# Last updated: Oct 3rd, 2024
# Author: Heather
# Libraries ---------------------------------------------------------------
# i pacmaned in all these scripts for u lauren
pacman::p_load(tidyverse, zoo, here, lubridate, data.table, fst)
source(here("code", "functions", "exposure_data_cleaning_helpers.R"))
# Constants ---------------------------------------------------------------
# just to make sure we make the same chunks each time
set.seed(7)
# we will make a version of the time series with customers out counts that
# have last obs carried forward, but we will impute a maximum of 4 hours
hour_thrshld <- dhours(4)
max_nas_to_impute <- hour_thrshld / dminutes(x = 5)
# fastest to do all years at once, taking advantage of data.table's
# infrastructure - excluding 2017 though bc it's garbage
#intervals_2017 <- generate_intervals(2017)
intervals_2018 <- generate_intervals(2018)
intervals_2019 <- generate_intervals(2019)
intervals_2020 <- generate_intervals(2020)
intervals_dt <-
data.table(date = c(
#intervals_2017,
intervals_2018,
intervals_2019,
intervals_2020
))
read in raw data with cleaned names
# read in raw data with cleaned names
pous_data <-
read_fst(
here(
"data",
"power_outage_exposure_data_cleaning_output",
"raw_with_fips.fst"
)
) |>
as.data.table()
# unique ids: five_digit_fips codes for county, and city_name and utility_name
# together for the city-utility unit
# have to expand this data in chunks due to R's vector limits
# want min number of chunks without exceeding the limit for max performance
# going with ~ 150 chunks; as few chunks as possible is best
pous_list <- sort(unique(pous_data$five_digit_fips))
pous_l_split <- split(pous_list, ceiling(seq_along(pous_list) / 15))
i = 1
# get chunk to process
pous_dat_chunk <-
get_chunk(raw_pous_data = pous_data, chunk_list = pous_l_split,
list_position = i)
# get city-utility id frame
city_utilities <- get_unique_city_utilities(pous_dat_chunk = pous_dat_chunk)
# get ten min time series
# intervals <-
#   data.table(date = generate_intervals(year))
ten_min_time_series <-
create_ten_min_series(id_frame = city_utilities,
intervals_dt = intervals_dt)
# id missing observations
pous_dat_chunk <-
id_missing_obs(pous_dat_chunk = pous_dat_chunk)
# expand the data to ten min intervals
pous_dat_chunk <-
expand_to_10_min_intervals(pous_dat_chunk = pous_dat_chunk)
# expand to a full year
pous_dat_chunk <-
expand_to_full_year(pous_dat_chunk = pous_dat_chunk,
city_utility_time_series = ten_min_time_series,
city_utilities = city_utilities)
# add an additional time series with locf to the data
pous_dat_chunk <-
add_locf_to_chunk_fill_in_gaps(pous_dat_chunk = pous_dat_chunk)
# replace -99 missing data indicators with NAs - need to edit this and see
# if it's working
pous_dat_chunk <- add_NAs_to_chunk(pous_dat_chunk = pous_dat_chunk)
# do locf for 4 hrs to replace NAs
pous_dat_chunk <-
add_locf_to_chunk_impute_4_hrs_forward(pous_dat_chunk = pous_dat_chunk,
max_nas_to_impute = max_nas_to_impute)
# note - if a gap is longer than 4 hrs, this doesn't impute anything at all
# it doesn't like, add the 4 hrs, and then stop, it just doesn't impute anything
# add customer served estimates by city_utility to chunk
pous_dat_chunk <- calculate_customer_served_est(pous_dat_chunk)
View(pous_dat_chunk)
# calculate person-time missing by city-utility ID and add it to data
pous_dat_chunk <- add_person_time_missing(pous_dat_chunk,
city_utilities = city_utilities)
View(pous)
View(pous_dat_chunk)
# want instead to sum customers out to hour here
pous_dat_chunk <- aggregate_customers_out_to_hour(pous_dat_chunk)
View(pous_dat_chunk)
# want to calculate person time missing at the hourly level here
pous_dat_chunk <- add_person_time_missing_hourly(pous_dat_chunk)
View(pous_dat_chunk[1:1000,])
sum(pous_dat_chunk$customers_out_hourly_locf != pous_dat_chunk$customers_out_hourly_no_locf)
sum(pous_dat_chunk$customers_out_hourly_locf != pous_dat_chunk$customers_out_hourly_no_locf, na.rm = T)
length(pous_dat_chunk$customers_out_hourly_locf)
861/1525632
# sum customers out to county
pous_dat_chunk <- sum_customers_out_to_county_hourly(pous_dat_chunk)
View(pous_dat_chunk)
# write the chunk
write_fst(
x = pous_dat_chunk,
path = here(
"data",
"power_outage_exposure_data_cleaning_output",
'hourly_county',
paste0(i,'_', "hourly_data.fst")
)
)
print(paste("Processed chunk", i))
print(dim(pous_dat_chunk))
print(sum(is.na(pous_dat_chunk$new_locf_rep)) /
length(pous_dat_chunk$new_locf_rep))
print(sum(is.na(pous_dat_chunk$new_locf_rep)) /
length(pous_dat_chunk$new_locf_rep), na.rm = T)
is.na(pous_dat_chunk$new_locf_rep)
sum(is.na(pous_dat_chunk$new_locf_rep))
print(sum(is.na(pous_dat_chunk$customers_out_hourly_locf)) /
length(pous_dat_chunk$customers_out_hourly_locf), na.rm = T)
