ggplot() +
geom_jitter(aes(x = day_of_week, y = mean_outages), alpha = 0.5, size = 3) +
facet_grid( ~ year)
p2
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(mean_outages = sum(exposed_8_hrs_0.005))
p2 <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(num_outages = sum(exposed_8_hrs_0.005))
p2 <- p2 %>% group_by(day_of_week) %>% summarize(n = mean(num_outages))
View(p2)
?wday
pacman::p_load(arrow, data.table, tidyverse, here, fst)
outages <- read_parquet(
here(
'data',
'power_outage_exposure_data_cleaning_output',
'analytic_exposure_data_all_years.parquet'
)
)
denoms <- read_fst(
here(
'data',
'power_outage_exposure_data_cleaning_output',
"county_customer_denoms_and_p_missing.fst"
)
)
outages <- outages %>% left_join(denoms)
outages <- outages %>% filter(percent_served > 0.5 & county_customers > 1000)
day_of_week <- outages %>%
mutate(day_of_week = lubridate::wday(day)) %>%
group_by(five_digit_fips, year, day_of_week) %>%
summarize(num_outages = sum(exposed_8_hrs_0.005))
day_of_week %>%
group_by(day_of_week) %>%
summarize(n = mean(num_outages)) %>%
knitr::kable()
p2 <- day_of_week %>%
ggplot() +
geom_jitter(aes(x = day_of_week, y = mean_outages),
alpha = 0.5,
size = 3) +
facet_grid(~ year)
p2
p2 <- day_of_week %>%
ggplot() +
geom_jitter(aes(x = day_of_week, y = num_outages),
alpha = 0.5,
size = 3) +
facet_grid(~ year)
p2
p2 <- day_of_week %>%
ggplot() +
geom_jitter(aes(x = day_of_week, y = num_outages),
alpha = 0.02,
size = 3) +
facet_grid(~ year)
p2
pacman::p_load(arrow, data.table, tidyverse, here, fst)
outages <- read_parquet(
here(
'data',
'power_outage_exposure_data_cleaning_output',
'analytic_exposure_data_all_years.parquet'
)
)
min(outages$day)
max(outages$day)
i <- outages %>% filter(year == 2019)
sum(i$exposed_8_hrs_0.005)
sum(i$exposed_8_hrs_0.005)/length(i$exposed_8_hrs_0.005)
# Expand Out Power Outages
# This script expands power outage data into a time series, from its raw
# form where the dataset only includes entries for changes in customers_out
# (see the POUS documentation for an explanation of how the raw data is
# structured). It does this one county at a time, and saves each expanded county
# in the 'hourly county' folder in the 'data' folder. Need to clean up
# ordering of this script.
# Last updated: Oct 3rd, 2024
# Author: Heather
# Libraries ---------------------------------------------------------------
# i pacmaned in all these scripts for u lauren
pacman::p_load(tidyverse, zoo, here, lubridate, data.table, fst)
source(here("code", "functions", "exposure_data_cleaning_helpers.R"))
# Constants ---------------------------------------------------------------
# just to make sure we make the same chunks each time
set.seed(7)
# we will make a version of the time series with customers out counts that
# have last obs carried forward, but we will impute a maximum of 4 hours
hour_thrshld <- dhours(4)
max_nas_to_impute <- hour_thrshld / dminutes(x = 5)
# fastest to do all years at once, taking advantage of data.table's
# infrastructure - excluding 2017 though bc it's garbage
#intervals_2017 <- generate_intervals(2017)
intervals_2018 <- generate_intervals(2018)
intervals_2019 <- generate_intervals(2019)
intervals_2020 <- generate_intervals(2020)
intervals_dt <-
data.table(date = c(
#intervals_2017,
intervals_2018,
intervals_2019,
intervals_2020
))
read in raw data with cleaned names
# read in raw data with cleaned names
pous_data <-
read_fst(
here(
"data",
"power_outage_exposure_data_cleaning_output",
"raw_with_fips.fst"
)
) |>
as.data.table()
# unique ids: five_digit_fips codes for county, and city_name and utility_name
# together for the city-utility unit
# have to expand this data in chunks due to R's vector limits
# want min number of chunks without exceeding the limit for max performance
# going with ~ 150 chunks; as few chunks as possible is best
pous_list <- sort(unique(pous_data$five_digit_fips))
pous_l_split <- split(pous_list, ceiling(seq_along(pous_list) / 15))
i = 1
# get chunk to process
pous_dat_chunk <-
get_chunk(raw_pous_data = pous_data, chunk_list = pous_l_split,
list_position = i)
# get city-utility id frame
city_utilities <- get_unique_city_utilities(pous_dat_chunk = pous_dat_chunk)
# get ten min time series
# intervals <-
#   data.table(date = generate_intervals(year))
ten_min_time_series <-
create_ten_min_series(id_frame = city_utilities,
intervals_dt = intervals_dt)
# id missing observations
pous_dat_chunk <-
id_missing_obs(pous_dat_chunk = pous_dat_chunk)
# expand the data to ten min intervals
pous_dat_chunk <-
expand_to_10_min_intervals(pous_dat_chunk = pous_dat_chunk)
# expand to a full year
pous_dat_chunk <-
expand_to_full_year(pous_dat_chunk = pous_dat_chunk,
city_utility_time_series = ten_min_time_series,
city_utilities = city_utilities)
# add an additional time series with locf to the data
pous_dat_chunk <-
add_locf_to_chunk_fill_in_gaps(pous_dat_chunk = pous_dat_chunk)
# replace -99 missing data indicators with NAs - need to edit this and see
# if it's working
pous_dat_chunk <- add_NAs_to_chunk(pous_dat_chunk = pous_dat_chunk)
# do locf for 4 hrs to replace NAs
pous_dat_chunk <-
add_locf_to_chunk_impute_4_hrs_forward(pous_dat_chunk = pous_dat_chunk,
max_nas_to_impute = max_nas_to_impute)
# note - if a gap is longer than 4 hrs, this doesn't impute anything at all
# it doesn't like, add the 4 hrs, and then stop, it just doesn't impute anything
# add customer served estimates by city_utility to chunk
pous_dat_chunk <- calculate_customer_served_est(pous_dat_chunk)
View(pous_dat_chunk)
# calculate person-time missing by city-utility ID and add it to data
pous_dat_chunk <- add_person_time_missing(pous_dat_chunk,
city_utilities = city_utilities)
View(pous)
View(pous_dat_chunk)
# want instead to sum customers out to hour here
pous_dat_chunk <- aggregate_customers_out_to_hour(pous_dat_chunk)
View(pous_dat_chunk)
# want to calculate person time missing at the hourly level here
pous_dat_chunk <- add_person_time_missing_hourly(pous_dat_chunk)
View(pous_dat_chunk[1:1000,])
sum(pous_dat_chunk$customers_out_hourly_locf != pous_dat_chunk$customers_out_hourly_no_locf)
sum(pous_dat_chunk$customers_out_hourly_locf != pous_dat_chunk$customers_out_hourly_no_locf, na.rm = T)
length(pous_dat_chunk$customers_out_hourly_locf)
861/1525632
# sum customers out to county
pous_dat_chunk <- sum_customers_out_to_county_hourly(pous_dat_chunk)
View(pous_dat_chunk)
# write the chunk
write_fst(
x = pous_dat_chunk,
path = here(
"data",
"power_outage_exposure_data_cleaning_output",
'hourly_county',
paste0(i,'_', "hourly_data.fst")
)
)
print(paste("Processed chunk", i))
print(dim(pous_dat_chunk))
print(sum(is.na(pous_dat_chunk$new_locf_rep)) /
length(pous_dat_chunk$new_locf_rep))
print(sum(is.na(pous_dat_chunk$new_locf_rep)) /
length(pous_dat_chunk$new_locf_rep), na.rm = T)
is.na(pous_dat_chunk$new_locf_rep)
sum(is.na(pous_dat_chunk$new_locf_rep))
print(sum(is.na(pous_dat_chunk$customers_out_hourly_locf)) /
length(pous_dat_chunk$customers_out_hourly_locf), na.rm = T)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a00_define_contiguous_US.R", echo=TRUE)
# This script estimates the number of electrical customers by state using the
# EIA data, so eventually we can generate county electrical customer estimates
# and coverage information.
# Author: Heather
# Last updated: Oct 2nd, 2024
# Libraries ---------------------------------------------------------------
pacman::p_load(tidyverse, readxl, here)
# Read --------------------------------------------------------------------
# read list of states we hope to include
state_fips_abbrev <- read_rds(here('data', 'cotus_state_fips_abbrev.RDS'))
# read customer counts
# list eia dataset names - these are datasets containing electrical customer
# counts by state
eia_sets <-
list.files(
here(
"data",
"power_outage_exposure_data_cleaning_raw_data",
"EIA"
),
full.names = TRUE
)
# read into a list for easy processing
cust_counts <-
lapply(eia_sets,
read_excel,
skip = 3,
col_names = F)
# set column types so that we can bind rows
cust_counts <-
lapply(X = cust_counts,
FUN = mutate,
across(everything(), as.character))
# remove column that weirdly appears in only one year so we can bind rows
# i guess 2019 was a long form year for the short-form utilities
cust_counts[[2]] <- cust_counts[[2]] %>% select(-c("...10"))
colnames(cust_counts[[2]]) <- colnames(cust_counts[[1]])
# get columns we want to count customers by state and year
cust_counts <- lapply(
X = cust_counts,
FUN = select,
year = "...1",
utility_name = "...3",
part = "...4",
customer_counts = "...24",
state_ab = "...7"
)
# set types for adding customer counts
cust_counts <-
bind_rows(cust_counts) %>%
mutate(customer_counts = as.numeric(na_if(x = customer_counts, y = '.'))) %>%
drop_na(customer_counts)
# get final customer counts by state and year - for correct counts, we drop part
# C, but total the other counts
cust_counts <- cust_counts %>%
filter(part != 'C') %>%
group_by(state_ab, year) %>%
summarise(eia_state_total_cust = sum(as.numeric(customer_counts),
na.rm = TRUE))
# all the states we expect to be there are there
# sum(state_fips_abbrev$state_ab %in%
#       unique(cust_counts$state_ab))/length(unique(cust_counts$state_ab))
# filter to only contiguous US states and add state fips
cust_counts <- cust_counts %>%
filter(state_ab %in% state_fips_abbrev$state_ab)
cust_counts <- cust_counts %>%
left_join(state_fips_abbrev) %>%
select(state_ab, state_fips, state_name, year, eia_state_total_cust) %>%
mutate(year = as.numeric(year))
# Write -------------------------------------------------------------------
write_rds(cust_counts,
here(
"data",
"power_outage_exposure_data_cleaning_output",
"eia_state_total_customers_by_year.RDS"
))
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a02_get_county_census_cust_est.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/a03_county_customer_census_estimates.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b01_read_and_clean.R", echo=TRUE)
oo <- read_fst(here("data", "power_outage_exposure_data_cleaning_output", "county_customer_denoms_and_p_missing.fst"))
View(oo)
oo <- oo %>% filter(year == 2018)
View(oo)
us_counties <- us_counties %>% left_join(oo)
# us counties
us_counties <- readRDS(here("data", "cotus_county_shp_w_fips.RDS"))
us_counties <- us_counties %>% left_join(oo)
colnames(oo)
colnames(us_counties)
pacman::p_load(here, tidyverse, sf, ggthemes, viridis, data.table, fst)
us_counties <- us_counties %>% left_join(oo)
p1 <- us_counties |>
filter(year == 2018) |>
#filter(p_present > 0.5) |>
ggplot() +
geom_sf(data = under_layer) +
geom_sf(aes(fill = p_present)) +
scale_fill_viridis_c(
name = "Proportion of person-time not missing in POUS dataset;
grey areas are missing more than 50% of ten-minute person-periods",
limits = c(0.5, 1)
) +
theme_map() +
ggtitle(
"2018 person-time coverage in the POUS dataset"
)
under_layer <- st_geometry(us_counties)
p1 <- us_counties |>
filter(year == 2018) |>
#filter(p_present > 0.5) |>
ggplot() +
geom_sf(data = under_layer) +
geom_sf(aes(fill = p_present)) +
scale_fill_viridis_c(
name = "Proportion of person-time not missing in POUS dataset;
grey areas are missing more than 50% of ten-minute person-periods",
limits = c(0.5, 1)
) +
theme_map() +
ggtitle(
"2018 person-time coverage in the POUS dataset"
)
ggsave(
here('figures', 'figures_output', 'person_time_coverage_hrs_oct_26.pdf'),
device = "pdf",
width = 14,
height = 12
)
colnames(us_counties)
p1 <- us_counties |>
filter(year == 2018) |>
#filter(p_present > 0.5) |>
ggplot() +
geom_sf(data = under_layer) +
geom_sf(aes(fill = percent_served)) +
scale_fill_viridis_c(
name = "Proportion of person-time not missing in POUS dataset;
grey areas are missing more than 50% of ten-minute person-periods",
limits = c(0.5, 1)
) +
theme_map() +
ggtitle(
"2018 person-time coverage in the POUS dataset"
)
ggsave(
here('figures', 'figures_output', 'person_time_coverage_hrs_oct_26.pdf'),
device = "pdf",
width = 14,
height = 12
)
# Plot county person-coverage for the POUS dataset in 2018
# Author: Heather
# Last updated: Oct 7th, 2024
# This runs on Heather's computer.
# Libraries ---------------------------------------------------------------
pacman::p_load(here, tidyverse, sf, ggthemes, viridis, data.table, fst)
# Read --------------------------------------------------------------------
# pous
cols_to_read <- c(
"clean_state_name",
"clean_county_name",
"five_digit_fips",
"year",
"customers_served_estimate_to_use",
"p_present"
)
pous <- read_fst(
here(
"data",
"power_outage_exposure_data_cleaning_output",
"hourly_data_with_coverage_exclusions.fst"
),
columns = cols_to_read
) |> distinct()
# us counties
us_counties <- readRDS(here("data", "cotus_county_shp_w_fips.RDS"))
us_counties <- us_counties |> left_join(pous)
# Plot --------------------------------------------------------------------
us_counties <- us_counties %>%
mutate(p_present = case_when(p_present > 1 ~ 1,
p_present < 0 ~ 0,
is.na(p_present) ~ 0,
T ~ p_present))
tables <- us_counties %>% filter(year == 2018) %>% mutate(percentage_full = case_when(p_present == 1 ~ 1,
T ~ 0),
percentage_20p = case_when(p_present > 0.2 ~ 1,
T ~ 0))
o <- tables %>% st_drop_geometry() %>% group_by(percentage_full) %>% summarize(n = mean(p_present))
View(o)
o <- tables %>% st_drop_geometry() %>% filter(p_present > 0.5) %>% group_by(percentage_full) %>% summarize(n = mean(p_present), l = n())
# Plot the number of hours without power throughout 2018 and also the number of
# discrete power outages based on the definition we've used
# Runs on Heather's computer
# Libraries ---------------------------------------------------------------
pacman::p_load(here, tidyverse, data.table, fst, sf, ggthemes)
# Read --------------------------------------------------------------------
# denoms
denoms <- read_fst(
here(
'data',
'power_outage_exposure_data_cleaning_output',
'county_customer_denoms_and_p_missing.fst'
)
)
# exposure data based on binary definition at different durations and cut
# points
all_days <- read_fst(
here(
"data",
"power_outage_exposure_data_cleaning_output",
"all_days_exposed_unexposed.fst"
)
)
all_days <- all_days %>%
filter(lubridate::year(day) == 2018) %>%
group_by(five_digit_fips) %>%
summarise(across(starts_with("exposed"), sum, na.rm = TRUE))
# us counties
us_counties <- readRDS(here("data", "cotus_county_shp_w_fips.RDS"))
us_counties <- us_counties |> left_join(all_days)
us_counties <- us_counties |> left_join(denoms)
# Plot --------------------------------------------------------------------
under_layer <- st_geometry(us_counties)
dif_exposure <- colnames(us_counties)[5:16]
pdf(
here("figures", "figures_output", "power_outage_rates_2018.pdf"),
width = 14,
height = 12,
compress = T
)
for (exp_col in dif_exposure){
p1 <- us_counties |>
filter(p_present > 0.5) |>
ggplot() +
#geom_sf(data = under_layer) +
geom_sf(aes(fill = !!sym(exp_col)), color = NA) +
scale_fill_viridis_c(
name = "Number of power outages in 2018",
) +
theme_map() +
ggtitle(
paste0("Number of power outages in 2018 for ", exp_col)
)
print(p1)
}
# Plot county person-coverage for the POUS dataset in 2018
# Author: Heather
# Last updated: Oct 7th, 2024
# This runs on Heather's computer.
# Libraries ---------------------------------------------------------------
pacman::p_load(here, tidyverse, sf, ggthemes, viridis, data.table, fst)
# Read --------------------------------------------------------------------
# pous
cols_to_read <- c(
"clean_state_name",
"clean_county_name",
"five_digit_fips",
"year",
"customers_served_estimate_to_use",
"p_present"
)
pous <- read_fst(
here(
"data",
"power_outage_exposure_data_cleaning_output",
"hourly_data_with_coverage_exclusions.fst"
),
columns = cols_to_read
) |> distinct()
# us counties
us_counties <- readRDS(here("data", "cotus_county_shp_w_fips.RDS"))
us_counties <- us_counties |> left_join(pous)
# Plot --------------------------------------------------------------------
us_counties <- us_counties %>%
mutate(p_present = case_when(p_present > 1 ~ 1,
p_present < 0 ~ 0,
is.na(p_present) ~ 0,
T ~ p_present))
under_layer <- st_geometry(us_counties)
p1 <- us_counties |>
filter(year == 2018) |>
#filter(p_present > 0.5) |>
ggplot() +
geom_sf(data = under_layer) +
geom_sf(aes(fill = p_present)) +
scale_fill_viridis_c(
name = "Proportion of person-time not missing in POUS dataset;
grey areas are missing more than 50% of ten-minute person-periods",
limits = c(0, 1)
) +
theme_map() +
ggtitle(
"2018 person-time coverage in the POUS dataset"
)
ggsave(
here('figures', 'figures_output', 'person_time_coverage_hrs_oct_26.png'),
width = 14,
height = 12
)
tables <- us_counties %>%
filter(year == 2018) %>%
mutate(
percentage_full = case_when(p_present == 1 ~ 1, T ~ 0),
percentage_20p = case_when(p_present > 0.2 ~ 1, T ~ 0)
)
hist(tables$p_present)
sum(tables$p_present==1)
sum(tables$p_present < 0.5)
View(tables)
