'local_data',
'effect_mod',
'nhgis0011_ds244_20195_county.csv'
))
acs_dat <- read_csv(here(
'local_data',
'effect_mod',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
here()
acs_dat <- read_csv(here(
'local_data',
'effect_mod_data',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
View(acs_data)
View(acs_dat)
# Load and clean the ACS data that we'll use as poverty measures.
# Author: Heather
# Date: Nov. 6th, 2024
# Libraires ---------------------------------------------------------------
pacman::p_load(tidyverse, here)
source(
here(
'code',
'data_prep',
'effect_mod_data_prep',
'effect_mod_data_cleaning_helpers.R'
)
)
acs_dat <- read_csv(here(
'local_data',
'effect_mod_data',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
acs_dat <- acs_dat %>%
mutate(total_below_pov = sum(ALWVE002 + ALWVE003),
p_below_pov = total_below_pov / ALWVE001)
glimpse(acs_dat)
acs_dat <- acs_dat %>% mutate(pov_quartile = find_quartiles(p_below_pov))
View(acs_dat)
# Load and clean the ACS data that we'll use as poverty measures.
# Author: Heather
# Date: Nov. 6th, 2024
# Libraires ---------------------------------------------------------------
pacman::p_load(tidyverse, here)
source(
here(
'code',
'data_prep',
'effect_mod_data_prep',
'effect_mod_data_cleaning_helpers.R'
)
)
# Read --------------------------------------------------------------------
acs_dat <- read_csv(here(
'local_data',
'effect_mod_data',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
# going to calculate total number of people making below poverty line
# as a percentage of total county pop
# calculate quartiles of that
# and then compare 1 to 4th quartile
acs_dat <- acs_dat %>%
mutate(total_below_pov = sum(ALWVE002 + ALWVE003),
p_below_pov = total_below_pov / ALWVE001,
five_digit_fips = paste0(STATEA, COUNTYA)) %>%
select(five_digit_fips, total_below_pov, p_below_pov)
acs_dat <- acs_dat %>% mutate(pov_quartile = find_quartiles(p_below_pov))
View(acs_dat)
hist(acs_dat$pov_quartile)
length(unique(acs_dat$five_digit_fips))
acs_dat <- acs_dat %>% filter(five_digit_fips %in% fips$five_digit_fips)
# Load and clean the ACS data that we'll use as poverty measures.
# Author: Heather
# Date: Nov. 6th, 2024
# Libraires ---------------------------------------------------------------
pacman::p_load(tidyverse, here)
source(
here(
'code',
'data_prep',
'effect_mod_data_prep',
'effect_mod_data_cleaning_helpers.R'
)
)
# Read --------------------------------------------------------------------
fips <- read_rds(here('data_for_upload', 'cotus_county_list_of_fips.RDS'))
acs_dat <- read_csv(here(
'local_data',
'effect_mod_data',
'nhgis0011_csv',
'nhgis0011_ds244_20195_county.csv'
))
# going to calculate total number of people making below poverty line
# as a percentage of total county pop
# calculate quartiles of that
# and then compare 1 to 4th quartile
acs_dat <- acs_dat %>%
mutate(total_below_pov = sum(ALWVE002 + ALWVE003),
p_below_pov = total_below_pov / ALWVE001,
five_digit_fips = paste0(STATEA, COUNTYA)) %>%
select(five_digit_fips, total_below_pov, p_below_pov)
acs_dat <-
acs_dat %>%
mutate(pov_quartile = find_quartiles(p_below_pov))
acs_dat <- acs_dat %>% filter(five_digit_fips %in% fips$five_digit_fips)
write_rds(acs_dat,
here('data_for_upload', 'effect_mod_data', 'pov_measures.RDS'))
View(acs_dat)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/poverty_measures.R", echo=TRUE)
View(acs_dat)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/effect_mod_data_prep/03_clean_temp.R", echo=TRUE)
pacman::p_load(here, tidyverse, data.table, fst)
write_fst(
days_over_thresholds,
here(
'data_for_upload',
'effect_mod_data',
'hot_and_cold_days.fst'
)
)
pacman::p_load(tidyverse, readxl, here)
# read customer counts
# list eia dataset names - these are datasets containing electrical customer
# counts by state
eia_sets <-
list.files(
here(
"data_for_upload",
"power_outage_exposure_data_cleaning_raw_data",
"EIA"
),
full.names = TRUE
)
View(eia_sets)
# read into a list for easy processing
cust_counts <-
lapply(eia_sets,
read_excel,
skip = 3,
col_names = F)
View(cust_counts)
View(cust_counts[[1]])
pacman::p_load(tidyverse, here, janitor, tidytext, snakecase, readxl,
data.table, fst)
# raw data from POUS
raw_pous_read <-
fread(
here(
"local_data",
"power_outage_exposure_data_cleaning_raw_data",
"POUS_Export_Raw_CityByUtility_20170101_20201231.csv"
)
)#, n_max = 10000)
glimpse(raw_pous_read)
o <- raw_pous_read %>% filter(StateName == 'PuertoRico')
sort(unique(raw_pous_read$StateName))
# Print the summary table
print(summary_table)
source("~/.active-rstudio-document", echo=TRUE)
library(dplyr)
library(tidyr)
# Example dataset
data <- data.frame(
five_digit_fips = sample(10000:99999, 1000, replace = TRUE),
day = sample(seq.Date(as.Date('2020-01-01'), as.Date('2020-12-31'), by="day"), 1000, replace = TRUE),
exposed_8_hrs_0.01 = runif(1000, 0, 1),
max_temp = runif(1000, 50, 100),
precip = runif(1000, 0, 10),
wind_speed = runif(1000, 0, 20),
n_benes_older_75 = sample(0:1, 1000, replace = TRUE),
dme_quartile = sample(1:4, 1000, replace = TRUE),
pov_quartile = sample(1:4, 1000, replace = TRUE)
)
# Function to calculate quartiles
calculate_quartiles <- function(data, group_var, exposure_var) {
data %>%
group_by(!!sym(group_var)) %>%
summarize(
Q1 = quantile(!!sym(exposure_var), 0.25, na.rm = TRUE),
Q2 = quantile(!!sym(exposure_var), 0.50, na.rm = TRUE),
Q3 = quantile(!!sym(exposure_var), 0.75, na.rm = TRUE),
Q4 = quantile(!!sym(exposure_var), 1.00, na.rm = TRUE)
) %>%
ungroup()
}
# Calculate quartiles for the entire dataset
overall_quartiles <- calculate_quartiles(data, "five_digit_fips", "exposed_8_hrs_0.01")
# Calculate quartiles for each subgroup
age_quartiles <- calculate_quartiles(data, "n_benes_older_75", "exposed_8_hrs_0.01")
dme_quartiles <- calculate_quartiles(data, "dme_quartile", "exposed_8_hrs_0.01")
pov_quartiles <- calculate_quartiles(data, "pov_quartile", "exposed_8_hrs_0.01")
wind_quartiles <- calculate_quartiles(data, "wind_speed", "exposed_8_hrs_0.01")
precip_quartiles <- calculate_quartiles(data, "precip", "exposed_8_hrs_0.01")
temp_quartiles <- calculate_quartiles(data, "max_temp", "exposed_8_hrs_0.01")
# Combine all quartiles into a single table
summary_table <- bind_rows(
overall_quartiles %>% mutate(Group = "Overall"),
age_quartiles %>% mutate(Group = ifelse(n_benes_older_75 == 1, "Age >= 75", "Age < 75")),
dme_quartiles %>% mutate(Group = paste("DME Quartile", dme_quartile)),
pov_quartiles %>% mutate(Group = paste("Poverty Quartile", pov_quartile)),
wind_quartiles %>% mutate(Group = "Wind Speed Quartiles"),
precip_quartiles %>% mutate(Group = "Precipitation Quartiles"),
temp_quartiles %>% mutate(Group = "Temperature Quartiles")
) %>%
select(Group, Q1, Q2, Q3, Q4)
# Print the summary table
print(summary_table)
View(summary_table)
pacman::p_load(here, tidyverse, data.table, fst, sf, ggthemes)
# denoms
denoms <- read_fst(
here(
'data',
'power_outage_exposure_data_cleaning_output',
'county_customer_denoms_and_p_missing.fst'
)
)
# exposure data based on binary definition at different durations and cut
# points
all_days <- read_fst(
here(
"data",
"power_outage_exposure_data_cleaning_output",
"all_days_exposed_unexposed.fst"
)
)
here()
# denoms
denoms <- read_fst(
here(
'local_data',
'power_outage_exposure_data_cleaning_output',
'county_customer_denoms_and_p_missing.fst'
)
)
# exposure data based on binary definition at different durations and cut
# points
all_days <- read_fst(
here(
"local_data",
"power_outage_exposure_data_cleaning_output",
"all_days_exposed_unexposed.fst"
)
)
all_days
mean(all_days$exposed_8_hrs_0.005)
sd(all_days$exposed_8_hrs_0.005)
pacman::p_load(here, tidyverse, data.table, fst, arrow)
i <- read_parquet(
here(
'data_for_upload',
"power_outage_exposure_data_cleaning_output",
"analytic_exposure_data_all_years.parquet"
)
)
i
i <- i %>% group_by(five_digit_fips) %>% summarize(n = sum(exposed_8_hrs_0.005))
i
mean(i$n)
sd(i$n)
View(i)
100/3
p <- read_parquet(
here(
'data_for_upload',
"power_outage_exposure_data_cleaning_output",
"analytic_exposure_data_2018.parquet"
)
)
View(p)
o <- p %>% group_by(clean_state_name, clean_county_name, five_digit_fips) %>% summarize(n = sum(exposed_8_hrs_0.005))
View(o)
mean(o$n)
sd(o$n)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b03_attach_denoms.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b04_id_outages_continuous_measures.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b05_identify_daily_binary_exposure.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b06_identify_outages_percentile.R", echo=TRUE)
source("/Volumes/squirrel-utopia/power_outage_national_cvd_hosp/code/data_prep/power_outage_exposure_data_prep/b07_create_exposure_data_for_upload.R", echo=TRUE)
install.packages("ncdf4")
install.packages("ncdf4")
pacman::p_load(ncdt4, data.table, tidyverse, here)
vs <- nc_open(here("local_data", "gridMET", 'vs_2018.nc'))
pacman::p_load(ncdt4, data.table, tidyverse, here)
vs <- nc_open(here("local_data", "gridMET", 'vs_2018.nc'))
pacman::p_load(ncdf4, data.table, tidyverse, here)
vs <- nc_open(here("local_data", "gridMET", 'vs_2018.nc'))
vs
county_shp <- readRDS(here('local_data', "cotus_county_shp_w_fips.RDS"))
st_crs(county_shp)
pacman::p_load(ncdf4, data.table, tidyverse, here, sf)
st_crs(county_shp)
vs <- st_transform(vs, st_crs(county_shp))
l <- brick(vs)
pacman::p_load(ncdf4, data.table, tidyverse, here, sf, raster)
l <- brick(vs)
l <- raster::brick(vs)
l <- raster::brick(here("local_data", "gridMET", 'vs_2018.nc'))
l
raster_brick <- raster::brick(here("local_data", "gridMET", 'vs_2018.nc'))
raster_list <- lapply(1:nlayers(raster_brick), function(i) raster(raster_brick, layer = i))
plot(raster_list[[1]])
crs(raster_brick)
crs(raster_brick[[1]])
county_shp <- st_transform(county_shp, crs(raster_brick[[1]]))
plot(county_shp)
crs(county_shp)
st_crs(county_shp)
pacman::p_load(ncdf4, data.table, tidyverse, here, sf, raster)
county_shp <- readRDS(here('local_data', "cotus_county_shp_w_fips.RDS"))
vs <- nc_open(here("local_data", "gridMET", 'vs_2018.nc'))
precip <- nc_open(here("local_data", "gridMET", "pr_2018.nc"))
tmax <- nc_open(here("local_data", "gridMET", "tmmx_2018.nc"))
st_crs(county_shp)
county_shp <- st_transform(county_shp, crs(raster_brick[[1]]))
raster_brick <- raster::brick(here("local_data", "gridMET", 'vs_2018.nc'))
vs <- raster::brick(here("local_data", "gridMET", 'vs_2018.nc'))
county_shp <- st_transform(county_shp, crs(raster_brick[[1]]))
st_crs(county_shp)
mean_values <- extract(vs[[1]], county_shp, fun = mean, na.rm = TRUE, df = TRUE)
glimpse(mean_values)
i <- cbind(county_shp, mean_values)
View(st_drop_geometry(i))
pacman::p_load(sf, here, tidyverse, data.table, sfarrow)
# This script creates files that have
# a) a crosswalk of state fips codes, county fips codes, state abbreviations,
# and state names for counties we wish to capture in the POUS dataset because
# they are in the contiguous US
# b) a list of counties and five-digit county fips codes that are in the
# contiguous US
# c) a shapefile of county boundaries for those counties in the contiguous US
# that we're aiming to include
# Last updated: Oct 3rd, 2024
# Author: Heather
pacman::p_load(sf, here, tidyverse, data.table, sfarrow)
# set shapefile for whole project
# states we wish to include: just being very clear about this list
cotus_state_fips_abbrev <- data.frame(
state_fips =
c("01", "04", "05", "06", "08", "09", "10", "12", "13", "16", "17", "18",
"19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30",
"31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42",
"44", "45", "46", "47", "48", "49", "50", "51", "53", "54", "55", "56",
"02", "15", "11"),
state_ab =
c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "ID", "IL", "IN",
"IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT",
"NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA",
"RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY",
"AK", "HI", "DC"),
state_name =
c("Alabama", "Arizona", "Arkansas", "California", "Colorado", "Connecticut",
"Delaware", "Florida", "Georgia", "Idaho", "Illinois", "Indiana", "Iowa",
"Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts",
"Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska",
"Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York",
"North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
"Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
"Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming",
"Alaska", "Hawaii", "District of Columbia"),
stringsAsFactors = FALSE
)
write_rds(cotus_state_fips_abbrev,
here("data_for_upload", 'cotus_state_fips_abbrev.RDS'))
# get and filter shapefile to that set of counties
county_shp <- tigris::counties(year = 2020) # get
county_shp <- county_shp |> # save relevant info only
mutate(five_digit_fips = paste0(STATEFP, COUNTYFP)) |>
select(five_digit_fips,
state_fips = STATEFP,
county_fips = COUNTYFP,
county_name = NAME)
pacman::p_load(sf, here, tidyverse, data.table, sfarrow)
# set shapefile for whole project
# states we wish to include: just being very clear about this list
cotus_state_fips_abbrev <- data.frame(
state_fips =
c("01", "04", "05", "06", "08", "09", "10", "12", "13", "16", "17", "18",
"19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30",
"31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42",
"44", "45", "46", "47", "48", "49", "50", "51", "53", "54", "55", "56",
"02", "15", "11"),
state_ab =
c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "ID", "IL", "IN",
"IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT",
"NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA",
"RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY",
"AK", "HI", "DC"),
state_name =
c("Alabama", "Arizona", "Arkansas", "California", "Colorado", "Connecticut",
"Delaware", "Florida", "Georgia", "Idaho", "Illinois", "Indiana", "Iowa",
"Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts",
"Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska",
"Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York",
"North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
"Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
"Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming",
"Alaska", "Hawaii", "District of Columbia"),
stringsAsFactors = FALSE
)
write_rds(cotus_state_fips_abbrev,
here("data_for_upload", 'cotus_state_fips_abbrev.RDS'))
# get and filter shapefile to that set of counties
county_shp <- tigris::counties(year = 2020) # get
county_shp <- county_shp |> # save relevant info only
mutate(five_digit_fips = paste0(STATEFP, COUNTYFP)) |>
select(five_digit_fips,
state_fips = STATEFP,
county_fips = COUNTYFP,
county_name = NAME)
# get and filter shapefile to that set of counties
county_shp <- tigris::counties(year = 2020) # get
county_shp
county_shp <- county_shp |> # save relevant info only
mutate(five_digit_fips = paste0(STATEFP, COUNTYFP)) |>
select(five_digit_fips,
state_fips = STATEFP,
county_fips = COUNTYFP,
county_name = NAME)
county_shp <- county_shp |> # save relevant info only
mutate(five_digit_fips = paste0(STATEFP, COUNTYFP)) |>
tidyverse::select(five_digit_fips,
state_fips = STATEFP,
county_fips = COUNTYFP,
county_name = NAME)
county_shp <- county_shp |> # save relevant info only
mutate(five_digit_fips = paste0(STATEFP, COUNTYFP)) |>
dplyr::select(five_digit_fips,
state_fips = STATEFP,
county_fips = COUNTYFP,
county_name = NAME)
pacman::p_load(sf, here, tidyverse, data.table, sfarrow)
st_write_parquet(county_shp, here('local_data', 'cotus_county_shp_w_fips.parquet'))
# This script creates files that have
# a) a crosswalk of state fips codes, county fips codes, state abbreviations,
# and state names for counties we wish to capture in the POUS dataset because
# they are in the contiguous US
# b) a list of counties and five-digit county fips codes that are in the
# contiguous US
# c) a shapefile of county boundaries for those counties in the contiguous US
# that we're aiming to include
# Last updated: Oct 3rd, 2024
# Author: Heather
pacman::p_load(sf, here, tidyverse, data.table, sfarrow)
# set shapefile for whole project
# states we wish to include: just being very clear about this list
cotus_state_fips_abbrev <- data.frame(
state_fips =
c("01", "04", "05", "06", "08", "09", "10", "12", "13", "16", "17", "18",
"19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30",
"31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42",
"44", "45", "46", "47", "48", "49", "50", "51", "53", "54", "55", "56",
"02", "15", "11"),
state_ab =
c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "ID", "IL", "IN",
"IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT",
"NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA",
"RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY",
"AK", "HI", "DC"),
state_name =
c("Alabama", "Arizona", "Arkansas", "California", "Colorado", "Connecticut",
"Delaware", "Florida", "Georgia", "Idaho", "Illinois", "Indiana", "Iowa",
"Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts",
"Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska",
"Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York",
"North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
"Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
"Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming",
"Alaska", "Hawaii", "District of Columbia"),
stringsAsFactors = FALSE
)
write_rds(cotus_state_fips_abbrev,
here("data_for_upload", 'cotus_state_fips_abbrev.RDS'))
# get and filter shapefile to that set of counties
county_shp <- tigris::counties(year = 2020) # get
county_shp <- county_shp |> # save relevant info only
mutate(five_digit_fips = paste0(STATEFP, COUNTYFP)) |>
dplyr::select(five_digit_fips,
state_fips = STATEFP,
county_fips = COUNTYFP,
county_name = NAME)
county_shp <- county_shp |>
filter(state_fips %in% cotus_state_fips_abbrev$state_fips)
# reproject to albers
epsg_code <- 5070
county_shp <- st_transform(county_shp, crs = epsg_code)
# write
write_rds(county_shp, here("local_data", "cotus_county_shp_w_fips.RDS"))
st_write_parquet(county_shp, here('local_data', 'cotus_county_shp_w_fips.parquet'))
# also write backbone file with just fips and no geometry
county_list <- county_shp %>%
st_drop_geometry() %>%
distinct()
write_rds(county_list, here("data_for_upload", "cotus_county_list_of_fips.RDS"))
# write backbone with all fips and all dates in 2018
dates_2018 <- seq(as.Date("2018-01-01"), as.Date("2018-12-31"), by = "day")
panel_fips_2018 <-
CJ(five_digit_fips = county_list$five_digit_fips, date = dates_2018)
write_rds(panel_fips_2018, here("data_for_upload", "panel_for_2018.RDS"))
install.packages(MuMIn)
install.packages('MuMIn')
install.packages('MuMIn')
